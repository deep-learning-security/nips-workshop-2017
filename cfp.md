---
layout: page
title: Machine Learning and Computer Security Workshop
---

# Call for Paper

## Overview

While traditional computer security relies on well-defined attack
models and proofs of security, a science of security for machine
learning systems has proven more elusive. This is due to a number
of obstacles, including (1) the highly varied angles of attack
against ML systems, (2) the lack of a clearly defined attack
surface (because the source of the data analyzed by ML systems is
not easily traced), and (3) the lack of clear formal definitions
of security that are appropriate for ML systems. At the same
time, security of ML systems is of great import due the recent
trend of using ML systems as a line of defense against malicious
behavior (e.g., network intrusion, malware, and ransomware), as
well as the prevalence of ML systems as parts of sensitive and
valuable software systems (e.g. sentiment analyzers for
predicting stock prices). This workshop will bring together
experts from the computer security and machine learning
communities in an attempt to highlight recent work in this area,
as well as to clarify the foundations of secure ML and chart out
important directions for future work and cross-community
collaborations.

## Topics

We invite submissions on any aspect of machine learning that
relates to computer security. This includes, but is not limited
to:

* Case studies of machine learning used in cyber security, such as detection of spam, sybils, or malicious URLs
* Whitepapers proposing or building on formal threat models and definitions of security
* Training time attacks (e.g., data poisoning)
* Adversarial examples at test time
* Model stealing (e.g., for reconnaissance of a system before mounting an attack)
* Theoretical foundations of adversarially robust learning
* Formal verification of machine learning systems
* Identifying bugs in machine learning systems, especially if they present security vulnerabilities
* Strategic analysis of present or future security / misuse risks and how to prioritize them

Submissions should have a clear explanation of their relationship
to security, for instance by describing an [attack model](https://en.wikipedia.org/wiki/Attack_model) and the ways
in which the submitted work addresses such attacks. While not mandatory,
submissions are encouraged to take special care in facilitating
reproducibility of research results (e.g., by open-sourcing their code).

